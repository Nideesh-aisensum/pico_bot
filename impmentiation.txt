Let me search for how to use NVIDIA API with Kimi and GLM models.> âš ï¸ **Security Notice First!**
> You just publicly shared your NVIDIA API key (`nvapi-H3Ri...`). **Please revoke it immediately** and generate a new one at [build.nvidia.com](https://build.nvidia.com) â†’ Settings â†’ API Keys â†’ Delete â†’ Create New. Anyone who sees this chat could use and exhaust your credits.

---

Now here's the full integration guide using your (new) NVIDIA key with **Kimi K2.5** and **GLM-5** for a Telegram bot on Render.com:

---

## ðŸ§  What's Available on NVIDIA NIM

Both models are fully available on NVIDIA's API:

- **Kimi K2.5** â€” open-source native multimodal agentic model, supports both instant and thinking modes, 256K context
- **GLM-5 744B MoE** â€” enables efficient reasoning for complex systems and long-horizon agentic tasks
- Both use the same base URL: `https://integrate.api.nvidia.com/v1` and are OpenAI-compatible

---

## ðŸ“ Full Project Structure

```
picoclaw-telegram-bot/
â”œâ”€â”€ bot.py              â† main Telegram bot
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ render.yaml
â””â”€â”€ .env                â† never commit this!
```

---

## ðŸ“„ `bot.py` â€” Full Telegram Bot with NVIDIA + Kimi K2.5 / GLM-5

```python
import os
import logging
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
from openai import OpenAI
from http.server import HTTPServer, BaseHTTPRequestHandler
import threading

# â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TELEGRAM_TOKEN   = os.environ["TELEGRAM_TOKEN"]
NVIDIA_API_KEY   = os.environ["NVIDIA_API_KEY"]
ALLOWED_USER_ID  = int(os.environ.get("ALLOWED_USER_ID", "0"))  # 0 = allow all

# Choose your model:
# "moonshotai/kimi-k2.5"   â† multimodal, thinking mode
# "zhipuai/glm-5-plus"     â† GLM-5, great for reasoning
MODEL = os.environ.get("MODEL", "moonshotai/kimi-k2.5")

# â”€â”€â”€ NVIDIA Client (OpenAI-compatible) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key=NVIDIA_API_KEY
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Store conversation history per user
conversations = {}

# â”€â”€â”€ Health check server (keeps Render alive) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class HealthHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"OK - Bot is running!")
    def log_message(self, format, *args):
        pass  # suppress logs

def run_health_server():
    port = int(os.environ.get("PORT", 8080))
    server = HTTPServer(("0.0.0.0", port), HealthHandler)
    logger.info(f"Health server on port {port}")
    server.serve_forever()

# â”€â”€â”€ Bot Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    conversations[user_id] = []  # reset history
    await update.message.reply_text(
        f"ðŸ‘‹ Hello! I'm powered by **{MODEL}** via NVIDIA NIM.\n\n"
        "Just send me any message to chat!\n"
        "Use /reset to clear conversation history.\n"
        "Use /model to switch between Kimi K2.5 and GLM-5.",
        parse_mode="Markdown"
    )

async def reset(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    conversations[user_id] = []
    await update.message.reply_text("ðŸ”„ Conversation reset!")

async def switch_model(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global MODEL
    if MODEL == "moonshotai/kimi-k2.5":
        MODEL = "zhipuai/glm-5-plus"
        name = "GLM-5"
    else:
        MODEL = "moonshotai/kimi-k2.5"
        name = "Kimi K2.5"
    await update.message.reply_text(f"âœ… Switched to **{name}**!", parse_mode="Markdown")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    
    # Optional: restrict to only your user ID
    if ALLOWED_USER_ID != 0 and user_id != ALLOWED_USER_ID:
        await update.message.reply_text("â›” Unauthorized.")
        return

    user_text = update.message.text
    
    # Initialize history for new user
    if user_id not in conversations:
        conversations[user_id] = []

    # Add user message to history
    conversations[user_id].append({"role": "user", "content": user_text})

    # Show typing indicator
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action="typing")

    try:
        # Build request params
        params = {
            "model": MODEL,
            "messages": [
                {"role": "system", "content": "You are a helpful AI assistant."},
                *conversations[user_id][-10:]  # keep last 10 messages for context
            ],
            "max_tokens": 1024,
            "temperature": 1.0 if "kimi" in MODEL else 0.7,
        }

        # Enable thinking mode for Kimi K2.5
        if "kimi" in MODEL:
            params["extra_body"] = {"chat_template_kwargs": {"thinking": True}}

        response = client.chat.completions.create(**params)
        reply = response.choices[0].message.content

        # Save assistant reply to history
        conversations[user_id].append({"role": "assistant", "content": reply})

        # Split long messages (Telegram limit is 4096 chars)
        if len(reply) > 4096:
            for i in range(0, len(reply), 4096):
                await update.message.reply_text(reply[i:i+4096])
        else:
            await update.message.reply_text(reply)

    except Exception as e:
        logger.error(f"Error: {e}")
        await update.message.reply_text(f"âŒ Error: {str(e)}")

# â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    # Start health server in background thread
    health_thread = threading.Thread(target=run_health_server, daemon=True)
    health_thread.start()

    # Start Telegram bot
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("reset", reset))
    app.add_handler(CommandHandler("model", switch_model))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    logger.info(f"Bot started with model: {MODEL}")
    app.run_polling()
```

---

## ðŸ“„ `requirements.txt`

```
python-telegram-bot==20.7
openai>=1.30.0
```

---

## ðŸ“„ `Dockerfile`

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8080
CMD ["python", "bot.py"]
```

---

## ðŸ“„ `render.yaml` (optional â€” for one-click deploy)

```yaml
services:
  - type: web
    name: picoclaw-telegram-bot
    runtime: docker
    plan: free
    envVars:
      - key: TELEGRAM_TOKEN
        sync: false
      - key: NVIDIA_API_KEY
        sync: false
      - key: ALLOWED_USER_ID
        sync: false
      - key: MODEL
        value: moonshotai/kimi-k2.5
```

---

## ðŸš€ Deploy Steps

### 1. Push to GitHub
```bash
git init && git add .
git commit -m "nvidia kimi telegram bot"
git remote add origin https://github.com/YOUR_USER/your-bot.git
git push -u origin main
```

### 2. Deploy on Render
1. Go to **render.com** â†’ New â†’ Web Service
2. Connect your GitHub repo
3. Set environment variables:

| Key | Value |
|---|---|
| `TELEGRAM_TOKEN` | Your BotFather token |
| `NVIDIA_API_KEY` | Your **new** NVIDIA key |
| `ALLOWED_USER_ID` | Your Telegram numeric ID |
| `MODEL` | `moonshotai/kimi-k2.5` |

4. Runtime: **Docker**, Plan: **Free**
5. Click **Deploy** â€” get your URL like `https://your-bot.onrender.com`

### 3. Setup UptimeRobot (Keep-Alive)
- URL to ping: `https://your-bot.onrender.com`
- Interval: every **5 minutes**
- Done âœ…

---

## ðŸŽ® Bot Commands in Telegram

| Command | Action |
|---|---|
| `/start` | Start & reset history |
| `/reset` | Clear conversation |
| `/model` | Toggle between Kimi K2.5 â†” GLM-5 |
| Any text | Chat with the AI |

---

## âš¡ Key Differences: Kimi K2.5 vs GLM-5

| Feature | Kimi K2.5 | GLM-5 |
|---|---|---|
| Type | Multimodal VLM | Text/Reasoning MoE |
| Size | Large | 744B MoE |
| Context | 256K tokens | 128K tokens |
| Thinking Mode | âœ… Yes | âœ… Yes |
| Best for | Images + complex tasks | Long reasoning, coding |

Both are **free** on NVIDIA NIM's developer tier. Just **remember to revoke your leaked key first!** ðŸ”‘