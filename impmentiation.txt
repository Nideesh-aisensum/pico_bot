You're absolutely right! I completely ignored PicoClaw and just wrote a generic Python Telegram bot. Let me fix that.Good catch! Now I have the real PicoClaw config structure from GitHub. Here's the **correct, complete guide** â€” using PicoClaw natively with NVIDIA API for Kimi K2.5 or GLM models, deployed on Render.

---

## ğŸ—‚ï¸ Project Structure

```
picoclaw-nvidia/
â”œâ”€â”€ config.json          â† PicoClaw config (NVIDIA + Telegram)
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ start.sh
â””â”€â”€ render.yaml
```

---

## ğŸ“„ `config.json` â€” The Real PicoClaw Config

PicoClaw uses a `model_list` format where you define `model_name`, `model` (as `vendor/model-id`), `api_base`, and `api_key`. NVIDIA's API is OpenAI-compatible, so we point it there directly:

```json
{
  "agents": {
    "defaults": {
      "workspace": "/root/.picoclaw/workspace",
      "model": "kimi-k2.5",
      "max_tokens": 8192,
      "temperature": 1.0,
      "max_tool_iterations": 20
    }
  },
  "model_list": [
    {
      "model_name": "kimi-k2.5",
      "model": "openai/moonshotai/kimi-k2.5",
      "api_base": "https://integrate.api.nvidia.com/v1",
      "api_key": "YOUR_NEW_NVIDIA_API_KEY"
    },
    {
      "model_name": "glm-4",
      "model": "openai/zhipuai/glm-4-9b-chat",
      "api_base": "https://integrate.api.nvidia.com/v1",
      "api_key": "YOUR_NEW_NVIDIA_API_KEY"
    }
  ],
  "channels": {
    "telegram": {
      "enabled": true,
      "token": "YOUR_TELEGRAM_BOT_TOKEN",
      "allowFrom": ["YOUR_NUMERIC_TELEGRAM_USER_ID"]
    }
  },
  "tools": {
    "web": {
      "duckduckgo": {
        "enabled": true,
        "max_results": 5
      }
    }
  }
}
```

> âœ… `openai/` prefix tells PicoClaw to use the OpenAI-compatible protocol â€” which NVIDIA NIM fully supports. The `api_base` overrides the default OpenAI URL to point at NVIDIA.

---

## ğŸ“„ `Dockerfile`

```dockerfile
FROM debian:bookworm-slim

# Install curl for downloading PicoClaw binary + health checks
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Download PicoClaw binary (ARM64 or AMD64 based on build platform)
ARG TARGETARCH
RUN if [ "$TARGETARCH" = "arm64" ]; then \
      curl -L https://github.com/sipeed/picoclaw/releases/latest/download/picoclaw-linux-arm64 -o picoclaw; \
    else \
      curl -L https://github.com/sipeed/picoclaw/releases/latest/download/picoclaw-linux-amd64 -o picoclaw; \
    fi && chmod +x picoclaw

# Copy config
COPY config.json /root/.picoclaw/config.json

# Create workspace directory
RUN mkdir -p /root/.picoclaw/workspace

# Copy start script
COPY start.sh .
RUN chmod +x start.sh

EXPOSE 8080

CMD ["./start.sh"]
```

---

## ğŸ“„ `start.sh` â€” PicoClaw + Keep-Alive Health Server

```bash
#!/bin/bash

# Start a simple HTTP health server on PORT (for Render keep-alive)
PORT=${PORT:-8080}

# Tiny health check server using bash + netcat (no Python needed!)
health_server() {
  while true; do
    echo -e "HTTP/1.1 200 OK\r\nContent-Length: 2\r\n\r\nOK" | nc -l -p $PORT -q 1 2>/dev/null || true
  done
}

# Start health server in background
health_server &
echo "âœ… Health server started on port $PORT"

# Start PicoClaw gateway
echo "ğŸ¦ Starting PicoClaw with NVIDIA + Kimi K2.5..."
./picoclaw gateway
```

---

## ğŸ“„ `render.yaml`

```yaml
services:
  - type: web
    name: picoclaw-nvidia-bot
    runtime: docker
    plan: free
    envVars:
      - key: PORT
        value: "8080"
```

---

## ğŸš€ Deploy Steps

### 1. Replace secrets in `config.json`
Fill in:
- `YOUR_NEW_NVIDIA_API_KEY` â€” your new key (after revoking the leaked one)
- `YOUR_TELEGRAM_BOT_TOKEN` â€” from @BotFather
- `YOUR_NUMERIC_TELEGRAM_USER_ID` â€” from @userinfobot

### 2. Push to GitHub
```bash
git init
git add .
git commit -m "picoclaw nvidia kimi telegram"
git remote add origin https://github.com/YOUR_USER/picoclaw-bot.git
git push -u origin main
```

### 3. Deploy on Render
- New â†’ Web Service â†’ Connect GitHub repo
- Runtime: **Docker**, Plan: **Free**
- Deploy âœ…

### 4. Add UptimeRobot pinger
- URL: `https://your-app.onrender.com`
- Every 5 minutes â†’ bot stays alive 24/7

---

## ğŸ”„ Switching Between Kimi K2.5 and GLM

Both models are defined in `model_list`. To switch, just change the `"model"` in `agents.defaults`:

```json
"model": "kimi-k2.5"   â† uses NVIDIA Kimi K2.5
"model": "glm-4"       â† uses NVIDIA GLM-4
```

Or you can message PicoClaw in Telegram: `use model glm-4` â€” it understands natural language model switching via its agent tools.

---

## Summary of What's Different Now

| Before (wrong) | Now (correct) |
|---|---|
| Custom Python bot | **Real PicoClaw binary** in Go |
| Manual Telegram polling code | **PicoClaw `gateway` command** handles Telegram natively |
| OpenAI SDK manually | **PicoClaw `model_list`** config with NVIDIA base_url |
| No PicoClaw at all ğŸ˜… | ğŸ¦ Full PicoClaw with all its agent tools, web search, heartbeat, etc. |